{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ghuioio/IT4868/blob/main/CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_W6amJ4LTuU"
      },
      "source": [
        "You may also remember doing max pooling on images, in which we take the max value out of a block of pixels. This would shrink down the image so we could run convolutions and find patterns at multiple scales. We can also apply this operation to text. This time there is just one dimension and we do this across all channels.\n",
        "\n",
        "https://assets.website-files.com/5ac6b7f2924c652fd013a891/5d3773ce8654d7058e4a8f95_5b883c8db9decad53453c791_2018-08-28%252018.12.12.gif"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBaoyVnSL1H8"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj5iL1NfMSuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42c6ab05-3771-4fe8-fbda-ae51cf156687"
      },
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.6)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.4.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.1.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.24)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphamminhkhoi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CPKNHJtS8QR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c79ca13-c0e7-482d-c7c7-14606fe954a4"
      },
      "source": [
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "import tempfile\n",
        "import urllib.request\n",
        "\n",
        "\n",
        "IMDB_URL = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\"\n",
        "OUTPUT_NAME = \"aclImdb\"\n",
        "\n",
        "def download_and_extract_archive():\n",
        "    if os.path.exists(OUTPUT_NAME):\n",
        "        print(\"Imdb dataset download target exists at \" + OUTPUT_NAME)\n",
        "    else:\n",
        "        with urllib.request.urlopen(IMDB_URL) as response:\n",
        "            with tempfile.NamedTemporaryFile() as temp_archive:\n",
        "                temp_archive.write(response.read())\n",
        "                imdb_tar = shutil.unpack_archive(\n",
        "                    temp_archive.name, extract_dir=\".\", format=\"gztar\")\n",
        "\n",
        "    return\n",
        "\n",
        "download_and_extract_archive()\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imdb dataset download target exists at aclImdb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vu7UWfXg8xSa"
      },
      "source": [
        "## CNNs + Word Embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "829tO4C9DPF5"
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "sep = os.path.sep\n",
        "\n",
        "def load_imdb():\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "\n",
        "    path = os.path.join('aclImdb', 'train', 'pos', '')\n",
        "    X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
        "    y_train.extend([1 for _ in range(12500)])\n",
        "\n",
        "    path = os.path.join('aclImdb', 'train', 'neg', '')\n",
        "    X_train.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
        "    y_train.extend([0 for _ in range(12500)])\n",
        "\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "\n",
        "    path = os.path.join('aclImdb', 'test', 'pos', '')\n",
        "    X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
        "    y_test.extend([1 for _ in range(12500)])\n",
        "\n",
        "    path = os.path.join('aclImdb', 'test', 'neg', '')\n",
        "    X_test.extend([open(path + f).read() for f in os.listdir(path) if f.endswith('.txt')])\n",
        "    y_test.extend([0 for _ in range(12500)])\n",
        "\n",
        "    y_train = np.array(y_train, dtype=np.int32)\n",
        "    y_test = np.array(y_test, dtype=np.int32)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xHd_eRPGJWE"
      },
      "source": [
        "(X, y), (X_test, y_test) = load_imdb()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGj_yhM3XFII",
        "outputId": "349364fa-f9a4-41ed-b4ab-ecd3b4b80955"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "Vvdr0Gs9GKrs",
        "outputId": "d6a7ddac-69c1-4dc7-ff56-fc2e38990f34"
      },
      "source": [
        "X[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\"La Maman et la putain\" is the beautifulest film of all time. And what\\'s most moving about it may be the relation between reality and art the movie deals with, which is directly inspired by Proust\\'s \"A la Recherche du temps perdu\".<br /><br />Indeed, \"La Maman et la putain\" and \"In search of lost time\" apparently tell the same story : the one of the failure of love, which repeats itself endlessly. The first woman\\'s name is always Gilberte, and the second woman appears like a twisted and deformed double of Gilberte : Veronika is like a \"whore Gilberte\", beautiful like the night, whereas Gilberte was pure, and \"beautiful like the day\". After the failure of the first love, a second love begins, but this one is like already doomed by the first one. Veronika takes the place of Gilberte, in Alexandre\\'s life and in the movie. She progressively eclipses her, first by time to time, Gilberte\\'s still coming when Alexandre waits for Veronika,then totally. That shows it\\'s the same sad story repeating itself, the same \"unfaithful woman\", like Alexandre says, who appears endlessly - and unfaithful is for Proust the higher point in love, which makes it exist, but which also underlines its illusions.<br /><br />Art is what causes the passage between what\\'s outside - the illusion of love - to what\\'s inside, which is the truth, and is a learning of this truth. For instance, when Veronika notices the strange way Alexandre makes is bed, he answers that he saw it in a movie, and then, that a movie, \"it\\'s made for that, to learn how to live, how to make a bed\". Alexander wants to live like he was in a film, he wants his life to be art. <br /><br />This conception of art comes from Proust, with whom Eustache shares the same rejection of \"political art\" and realism in art. \"La Maman et la putain\" fights against a conception of art \"principaly political\" - see for example the ironical review of a political movie by Alexandre. Like Proust says : \"Art doesn\\'t care for all this proclamations, and only exists in silence.\" First of all, art is introspection. And that also why realism or naturalism is rejected : art needs to transform reality to exist. Proust writes : \"I discover the illusion of realism, which is a lie\". That\\'s why \"La Maman et la putain\" doesn\\'t hide its artificiality, underlines by the way the actors \"say\" their text : \"the more you seem artificial, the higher you go\", said Eustache.<br /><br />Eustache and Proust both share this idea that the artist is a \"translater\" of a inner truth. But, Alexandre failed where Eustache succeed. \"La Maman et la putain\" tells us the failure of a character to be what he truly is. You can sens the tragedy arise when you go further in the movie, which becomes saddest. You can see it in the face of Alexandre, who looks more and more like a living-dead. You can see it by the fact that the scenes become longer, and that after a while, nothing happens outside. At the end of the movie, when you see Alexandre writing, and Veronika asking if he\\'s writing his life,you can guess that he\\'s not, that even literature failed. The end of the movie shows the symbolic death of Alexander, who is smashes by the heaviness of reality. And in this tiny nurse\\'s room, Alexandre looks more like Albertine than Marcel.<br /><br />To explain this failure, we can say that Alexandre is a Balzac\\'s reader. In \"Forme et signification\", Jean Rousset explains that, in Proust\\'s, the readers of Balzac, who are Swann and Charlus, are unable of any artistic creation, because they\\'re stuck in reality, which they mistake with art. They see reality in art and \"are not aware of the transformations that necessarily exist between the life of an artist and his work, between reality and art\". And that\\'s exactly Alexandre. He claims for instance that he \"loves a woman for parallel reasons, because she played in a Bresson\\'s for example\". He\\'s like Swann, who falls in love with Odette because she looks like a Botticelli\\'s woman.<br /><br />\"Life is perhaps not my vocation\". This thought is indeed by Eustache, who committed suicide, even if it\\'s said by Alexandre. Nevertheless, there is a difference between Alexandre and Eustache : if Eustache is absolutely Alexandre, Alexandre is like a double without art, a horrible vision of the artist, which crystallizes his fears.<br /><br />By fallowing Veronika at the end of the movie, Alexandre is condemned to illusions. It\\'s death that remind me the last frames of the movie, in the face of Jean-Pierre Léaud as well as in the endless pucking of Veronika. Or maybe it is already hell that describes the end, like in Sarte\\'s \"Huit-Clot\", and absolutely not like in the final liberation of \"Le Temps retrouvé\". If Eustache had read Proust, Alexandre could never have finish the book , always perturbed by life and Veronika when he tries to read it at his apartment or in the cafés. \"La Maman et la Putain\" is like a inverse double of \"In search of lost times\", which tells how Alexander doesn\\'t become an artist, whereas \"A la Recherche du temps perdu\" tells how Marcel becomes a writer (Genette).<br /><br />If, like Baudelaire says, an artiste tells \"reality at the light of his dream\", it is his nightmare that Eustache tells us in \"La Maman et la putain\".'"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZNMM6PkGrqd"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 1 - 12356/12500, random_state = 42)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "id": "5QTFLjZlWces",
        "outputId": "ac6211d3-f280-4130-badc-b9ae0f7a9f90"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"I think this piece of garbage is the best proof that good ideas can be destroyed, why all the American animators thinks that the kids this days wants stupid GI JOE versions of good stories??? the Looney Tunes are some of the most beloved characters in history, but they weren't created to be Xtreme, i mean come on!!! Tiny Toons was a great example of how an old idea can be updated without loosing it's original charm, but this piece of garbage is just an example of stupid corporate decisions that only wants to create a cheap idiotic show that kids will love because hey!!! kids loves superheroes right??? the whole show is only a waste of time in which we see the new versions of the Looney Tunes but this time in superhero form, this doesn't sound too bad but the problem is that this show tries too hard to copy series like batman the animated series, or the new justice league, the result??? bad copies of flash (the road runner) or superman (who else??? bugs bunny) the problem is that Looney Tunes weren't meant to be dramatic, the were supposed to be funny!!!! as i said before this series sucks, and many people wonders why anime is taking all over the world??? this show tries to be dramatic and action packed, but that's something that few series and anime are able to do, if you want to see a good upgrade of an old show watch Tiny Toons, that's an example that it's possible to bring back to life old characters, but with a good story and respecting the original roots. too bad that show is already dead, another corporate wise decision i suppose.\""
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsfuHyHqWrBP",
        "outputId": "9eb7e32a-94bc-41b4-d150-88ccf1b272c5"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24712"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "WcnPPl0GTAhH",
        "outputId": "cd93eb9a-bf74-4035-9d67-02ceb61ca6e6"
      },
      "source": [
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding, LSTM\n",
        "from keras.layers import Conv1D, Flatten, MaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import numpy as np\n",
        "from keras.preprocessing import text\n",
        "wandb.init()\n",
        "config = wandb.config\n",
        "\n",
        "# set parameters:\n",
        "config.vocab_size = 1000\n",
        "config.maxlen = 1000\n",
        "config.batch_size = 32\n",
        "config.embedding_dims = 10\n",
        "config.filters = 32\n",
        "config.kernel_size = 3\n",
        "config.hidden_dims = 250\n",
        "config.epochs = 5\n",
        "\n",
        "\n",
        "tokenizer = text.Tokenizer(num_words=config.vocab_size)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_matrix(X_train)\n",
        "X_test = tokenizer.texts_to_matrix(X_test)\n",
        "\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=config.maxlen)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=config.maxlen)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(config.vocab_size,\n",
        "                    config.embedding_dims,\n",
        "                    input_length=config.maxlen))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Conv1D(config.filters,\n",
        "                 config.kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Conv1D(config.filters,\n",
        "                 config.kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu'))\n",
        "model.add(MaxPooling1D())\n",
        "model.add(Flatten())\n",
        "model.add(Dense(config.hidden_dims, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=config.batch_size,\n",
        "          epochs=config.epochs,\n",
        "          validation_data=(X_test, y_test), callbacks=[WandbCallback()])\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mphamminhkhoi\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/phamminhkhoi/uncategorized/runs/37lc5bc2\" target=\"_blank\">icy-pine-12</a></strong> to <a href=\"https://wandb.ai/phamminhkhoi/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "773/773 [==============================] - 71s 91ms/step - loss: 0.4916 - accuracy: 0.7450 - val_loss: 0.3720 - val_accuracy: 0.8336\n",
            "Epoch 2/5\n",
            "773/773 [==============================] - 70s 91ms/step - loss: 0.3775 - accuracy: 0.8307 - val_loss: 0.3571 - val_accuracy: 0.8386\n",
            "Epoch 3/5\n",
            "773/773 [==============================] - 70s 90ms/step - loss: 0.3535 - accuracy: 0.8460 - val_loss: 0.3622 - val_accuracy: 0.8385\n",
            "Epoch 4/5\n",
            "773/773 [==============================] - 69s 89ms/step - loss: 0.3320 - accuracy: 0.8551 - val_loss: 0.3494 - val_accuracy: 0.8472\n",
            "Epoch 5/5\n",
            "773/773 [==============================] - 68s 89ms/step - loss: 0.3046 - accuracy: 0.8712 - val_loss: 0.3611 - val_accuracy: 0.8397\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2aa1d13e50>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3XbXBzAuUx-g",
        "outputId": "3ebcb031-5477-4dd1-d26e-a247d2b63c59"
      },
      "source": [
        "import tensorflow as tf \n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc' ,tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
        "\n",
        "# fit the model\n",
        "history = model.fit(X_train, y_train,\n",
        "          batch_size=config.batch_size,\n",
        "          epochs=config.epochs,\n",
        "          validation_data=(X_test, y_test))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "773/773 [==============================] - 71s 89ms/step - loss: 0.1235 - acc: 0.9528 - precision_2: 0.9533 - recall_2: 0.9522 - val_loss: 0.5016 - val_acc: 0.8380 - val_precision_2: 0.8289 - val_recall_2: 0.8518\n",
            "Epoch 2/5\n",
            "773/773 [==============================] - 69s 89ms/step - loss: 0.1112 - acc: 0.9577 - precision_2: 0.9560 - recall_2: 0.9595 - val_loss: 0.5404 - val_acc: 0.8355 - val_precision_2: 0.8342 - val_recall_2: 0.8374\n",
            "Epoch 3/5\n",
            "773/773 [==============================] - 69s 89ms/step - loss: 0.0950 - acc: 0.9637 - precision_2: 0.9640 - recall_2: 0.9633 - val_loss: 0.5655 - val_acc: 0.8351 - val_precision_2: 0.8455 - val_recall_2: 0.8202\n",
            "Epoch 4/5\n",
            "773/773 [==============================] - 68s 89ms/step - loss: 0.0876 - acc: 0.9665 - precision_2: 0.9666 - recall_2: 0.9663 - val_loss: 0.5946 - val_acc: 0.8348 - val_precision_2: 0.8477 - val_recall_2: 0.8163\n",
            "Epoch 5/5\n",
            "773/773 [==============================] - 68s 88ms/step - loss: 0.0810 - acc: 0.9699 - precision_2: 0.9703 - recall_2: 0.9694 - val_loss: 0.6182 - val_acc: 0.8326 - val_precision_2: 0.8321 - val_recall_2: 0.8333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lri_HkMkVd4r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}